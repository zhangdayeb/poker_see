#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®æ—¶è¯†åˆ«æ¨é€ç³»ç»Ÿ - tui.py (æ··åˆè¯†åˆ«å™¨ç‰ˆæœ¬)
ä¸šåŠ¡é€»è¾‘:
1. è¯»å–æ‘„åƒå¤´é…ç½®
2. è½®è¯¢æ‹ç…§
3. è½®è¯¢è£å‰ª
4. è½®è¯¢æ··åˆè¯†åˆ« (YOLO + OCR + OpenCV)
5. ç»“æœåˆå¹¶ä¼˜åŒ–
6. è½®è¯¢æ¨é€
"""

import sys
import time
import json
import signal
import argparse
import threading
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional

# è·¯å¾„è®¾ç½®
def setup_project_paths():
    """è®¾ç½®é¡¹ç›®è·¯å¾„"""
    current_file = Path(__file__).resolve()
    project_root = current_file.parent
    
    project_root_str = str(project_root)
    if project_root_str not in sys.path:
        sys.path.insert(0, project_root_str)
    
    return project_root

PROJECT_ROOT = setup_project_paths()

class EnhancedTuiSystem:
    """å¢å¼ºç‰ˆå®æ—¶è¯†åˆ«æ¨é€ç³»ç»Ÿ - ä½¿ç”¨æ··åˆè¯†åˆ«å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        self.shutdown_requested = False
        
        # ç³»ç»Ÿé…ç½®
        self.config = {
            'recognition_interval': 3,  # æ¯è½®å¾ªç¯é—´éš”(ç§’)
            'camera_switch_delay': 1,   # æ‘„åƒå¤´åˆ‡æ¢å»¶è¿Ÿ(ç§’)
            'max_retry_times': 3,       # æœ€å¤§é‡è¯•æ¬¡æ•°
            'retry_delay': 2,           # é‡è¯•å»¶è¿Ÿ(ç§’)
            'enable_websocket': True,   # å¯ç”¨WebSocketæ¨é€
            'save_recognition_results': True,  # ä¿å­˜è¯†åˆ«ç»“æœ
            'enable_result_merging': True,  # å¯ç”¨ç»“æœåˆå¹¶
        }
        
        # æ··åˆè¯†åˆ«å™¨é…ç½®
        self.recognition_config = {
            # YOLOé…ç½®
            'yolo_enabled': True,
            'yolo_confidence_threshold': 0.3,
            'yolo_high_confidence_threshold': 0.8,
            
            # OCRé…ç½®
            'ocr_enabled': True,
            'ocr_confidence_threshold': 0.3,
            'ocr_prefer_paddle': True,
            
            # OpenCVèŠ±è‰²è¯†åˆ«é…ç½®
            'opencv_suit_enabled': True,
            'opencv_suit_confidence_threshold': 0.4,
            
            # èåˆç­–ç•¥é…ç½®
            'fusion_strategy': 'weighted',  # weighted, voting, priority
            'min_confidence_for_result': 0.3,
            'enable_result_validation': True,
            
            # æ€§èƒ½é…ç½®
            'debug_mode': False,
            'save_intermediate_results': False
        }
        
        # ç»“æœåˆå¹¶å™¨é…ç½®
        self.merger_config = {
            'min_confidence_threshold': 0.3,
            'high_confidence_threshold': 0.8,
            'conflict_resolution_strategy': 'highest_confidence',
            'enable_consistency_check': True,
            'quality_assessment_enabled': True,
            'duplicate_detection_enabled': True,
            'include_metadata': True,
            'include_quality_metrics': True
        }
        
        # æ‘„åƒå¤´é…ç½®
        self.enabled_cameras = []
        self.current_camera_index = 0
        
        # WebSocketå®¢æˆ·ç«¯
        self.websocket_client = None
        self.websocket_connected = False
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'start_time': datetime.now(),
            'total_cycles': 0,
            'camera_stats': {},  # æ¯ä¸ªæ‘„åƒå¤´çš„ç»Ÿè®¡
            'last_results': {},  # æœ€åä¸€æ¬¡è¯†åˆ«ç»“æœ
            'recognition_method_stats': {
                'yolo_complete': 0,      # YOLOå®Œæ•´è¯†åˆ«
                'hybrid_combined': 0,    # æ··åˆç»„åˆè¯†åˆ«
                'ocr_only': 0,          # ä»…OCRè¯†åˆ«
                'opencv_only': 0,       # ä»…OpenCVè¯†åˆ«
                'failed': 0             # è¯†åˆ«å¤±è´¥
            },
            'quality_stats': {
                'excellent': 0,    # ä¼˜ç§€
                'good': 0,         # è‰¯å¥½
                'average': 0,      # ä¸€èˆ¬
                'poor': 0,         # è¾ƒå·®
                'very_poor': 0     # å¾ˆå·®
            }
        }
        
        # æ˜¾ç¤ºçŠ¶æ€
        self.display_lock = threading.Lock()
        
        print("ğŸš€ å¢å¼ºç‰ˆå®æ—¶è¯†åˆ«æ¨é€ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ (ä½¿ç”¨æ··åˆè¯†åˆ«å™¨)")
    
    def step1_load_camera_config(self) -> bool:
        """æ­¥éª¤1: è¯»å–æ‘„åƒå¤´é…ç½®"""
        try:
            print("\nğŸ“· æ­¥éª¤1: è¯»å–æ‘„åƒå¤´é…ç½®")
            print("-" * 50)
            
            # ä½¿ç”¨é…ç½®ç®¡ç†å™¨
            from src.core.config_manager import get_all_cameras
            
            result = get_all_cameras()
            if result['status'] != 'success':
                print(f"âŒ è·å–æ‘„åƒå¤´é…ç½®å¤±è´¥: {result['message']}")
                return False
            
            cameras = result['data']['cameras']
            if not cameras:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°æ‘„åƒå¤´é…ç½®")
                return False
            
            # è¿‡æ»¤å¯ç”¨çš„æ‘„åƒå¤´
            self.enabled_cameras = [c for c in cameras if c.get('enabled', True)]
            if not self.enabled_cameras:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ‘„åƒå¤´")
                return False
            
            print(f"âœ… æ‰¾åˆ° {len(self.enabled_cameras)} ä¸ªå¯ç”¨çš„æ‘„åƒå¤´:")
            for i, camera in enumerate(self.enabled_cameras):
                camera_id = camera['id']
                
                # åˆå§‹åŒ–æ‘„åƒå¤´ç»Ÿè®¡
                self.stats['camera_stats'][camera_id] = {
                    'total_attempts': 0,
                    'successful_photos': 0,
                    'successful_recognitions': 0,
                    'successful_pushes': 0,
                    'last_photo_time': None,
                    'last_recognition_time': None,
                    'last_push_time': None,
                    'last_result': None,
                    'recognition_method_counts': {
                        'yolo_complete': 0,
                        'hybrid_combined': 0,
                        'ocr_only': 0,
                        'opencv_only': 0
                    },
                    'average_quality_score': 0.0,
                    'quality_history': []
                }
                
                print(f"   {i+1}. {camera['name']} (ID: {camera_id}) - IP: {camera['ip']}")
            
            return True
            
        except Exception as e:
            print(f"âŒ è¯»å–æ‘„åƒå¤´é…ç½®å¤±è´¥: {e}")
            return False
    
    def step2_take_photo(self, camera_id: str) -> Dict[str, Any]:
        """æ­¥éª¤2: æ‹ç…§"""
        try:
            # å¯¼å…¥æ‹ç…§æ§åˆ¶å™¨
            from src.processors.photo_controller import take_photo_by_id
            
            start_time = time.time()
            result = take_photo_by_id(camera_id)
            duration = time.time() - start_time
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats['camera_stats'][camera_id]['total_attempts'] += 1
            
            if result['status'] == 'success':
                self.stats['camera_stats'][camera_id]['successful_photos'] += 1
                self.stats['camera_stats'][camera_id]['last_photo_time'] = datetime.now().strftime('%H:%M:%S')
                
                return {
                    'success': True,
                    'file_path': result['data']['file_path'],
                    'filename': result['data']['filename'],
                    'file_size': result['data']['file_size'],
                    'duration': duration
                }
            else:
                return {
                    'success': False,
                    'error': result['message'],
                    'duration': duration
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'duration': 0
            }
    
    def step3_crop_images(self, image_path: str) -> Dict[str, Any]:
        """æ­¥éª¤3: è£å‰ªå›¾ç‰‡"""
        try:
            from src.processors.image_cutter import process_image
            
            start_time = time.time()
            success = process_image(image_path)
            duration = time.time() - start_time
            
            if success:
                # æŸ¥æ‰¾è£å‰ªåçš„å›¾ç‰‡
                image_file = Path(image_path)
                cut_dir = image_file.parent / "cut"
                
                if cut_dir.exists():
                    pattern = f"{image_file.stem}_*.png"
                    all_files = list(cut_dir.glob(pattern))
                    
                    # åˆ†ç¦»ä¸»å›¾ç‰‡å’Œå·¦ä¸Šè§’å›¾ç‰‡
                    main_files = [f for f in all_files if not f.name.endswith('_left.png')]
                    left_files = [f for f in all_files if f.name.endswith('_left.png')]
                    
                    main_files.sort(key=lambda x: x.name)
                    left_files.sort(key=lambda x: x.name)
                    
                    return {
                        'success': True,
                        'main_files': [str(f) for f in main_files],
                        'left_files': [str(f) for f in left_files],
                        'total_count': len(main_files),
                        'duration': duration
                    }
                else:
                    return {
                        'success': False,
                        'error': 'è£å‰ªç›®å½•ä¸å­˜åœ¨',
                        'duration': duration
                    }
            else:
                return {
                    'success': False,
                    'error': 'å›¾ç‰‡è£å‰ªå¤±è´¥',
                    'duration': duration
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'duration': 0
            }
    
    def step4_recognize_images_hybrid(self, camera_id: str, main_files: List[str], left_files: List[str]) -> Dict[str, Any]:
        """æ­¥éª¤4: æ··åˆè¯†åˆ«æ‰‘å…‹ç‰Œ"""
        try:
            from src.processors.poker_hybrid_recognizer import recognize_poker_card_hybrid
            
            position_results = {}
            successful_count = 0
            total_count = len(main_files)
            
            # åˆ›å»ºä¸»å›¾ç‰‡å’Œå·¦ä¸Šè§’å›¾ç‰‡çš„å¯¹åº”å…³ç³»
            main_to_left_map = self._create_image_mapping(main_files, left_files)
            
            start_time = time.time()
            
            for i, main_image_path in enumerate(main_files):
                position = self._extract_position_from_filename(Path(main_image_path).name)
                left_image_path = main_to_left_map.get(main_image_path)
                
                # ä½¿ç”¨æ··åˆè¯†åˆ«å™¨
                result = recognize_poker_card_hybrid(
                    main_image_path, 
                    left_image_path,
                    config=self.recognition_config
                )
                
                if result['success']:
                    successful_count += 1
                    
                    # ç»Ÿè®¡è¯†åˆ«æ–¹æ³•
                    self._update_recognition_method_stats(camera_id, result)
                
                position_results[position] = result
            
            duration = time.time() - start_time
            
            # æ›´æ–°ç»Ÿè®¡
            if successful_count > 0:
                self.stats['camera_stats'][camera_id]['successful_recognitions'] += 1
                self.stats['camera_stats'][camera_id]['last_recognition_time'] = datetime.now().strftime('%H:%M:%S')
            
            return {
                'success': successful_count > 0,
                'position_results': position_results,
                'successful_count': successful_count,
                'total_count': total_count,
                'success_rate': (successful_count / total_count * 100) if total_count > 0 else 0,
                'duration': duration
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'duration': 0
            }
    
    def step5_merge_results(self, camera_id: str, position_results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """æ­¥éª¤5: åˆå¹¶å’Œä¼˜åŒ–è¯†åˆ«ç»“æœ"""
        try:
            if not self.config['enable_result_merging']:
                # å¦‚æœç¦ç”¨åˆå¹¶ï¼Œç›´æ¥æ ¼å¼åŒ–ç»“æœ
                return self._format_recognition_results_simple(camera_id, position_results)
            
            from src.processors.poker_result_merger import merge_poker_recognition_results
            
            metadata = {
                'system_mode': 'realtime_push',
                'fusion_strategy': self.recognition_config['fusion_strategy'],
                'timestamp': datetime.now().isoformat()
            }
            
            start_time = time.time()
            merge_result = merge_poker_recognition_results(
                position_results,
                camera_id=camera_id,
                metadata=metadata,
                config=self.merger_config
            )
            duration = time.time() - start_time
            
            # æ›´æ–°è´¨é‡ç»Ÿè®¡
            if merge_result.get('success') and 'quality' in merge_result:
                self._update_quality_stats(camera_id, merge_result['quality'])
            
            # è½¬æ¢ä¸ºæ¨é€æ ¼å¼
            if merge_result.get('success'):
                formatted_result = self._convert_merge_result_to_push_format(camera_id, merge_result)
                formatted_result['merge_duration'] = duration
                return formatted_result
            else:
                return {
                    'success': False,
                    'error': merge_result.get('error', 'ç»“æœåˆå¹¶å¤±è´¥'),
                    'duration': duration
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'duration': 0
            }
    
    def step6_push_results(self, camera_id: str, formatted_results: Dict[str, Any]) -> Dict[str, Any]:
        """æ­¥éª¤6: æ¨é€è¯†åˆ«ç»“æœ"""
        try:
            if not self.config['enable_websocket']:
                return {'success': True, 'message': 'WebSocketæ¨é€å·²ç¦ç”¨', 'duration': 0}
            
            start_time = time.time()
            
            # æ¨é€åˆ°è¯†åˆ«ç»“æœç®¡ç†å™¨
            push_result = self._push_to_recognition_manager(formatted_results)
            
            duration = time.time() - start_time
            
            # æ›´æ–°ç»Ÿè®¡
            if push_result['success']:
                self.stats['camera_stats'][camera_id]['successful_pushes'] += 1
                self.stats['camera_stats'][camera_id]['last_push_time'] = datetime.now().strftime('%H:%M:%S')
            
            return {
                'success': push_result['success'],
                'message': push_result['message'],
                'duration': duration
            }
            
        except Exception as e:
            return {
                'success': False,
                'message': str(e),
                'duration': 0
            }
    
    def _create_image_mapping(self, main_files: List[str], left_files: List[str]) -> Dict[str, str]:
        """åˆ›å»ºä¸»å›¾ç‰‡å’Œå·¦ä¸Šè§’å›¾ç‰‡çš„å¯¹åº”å…³ç³»"""
        mapping = {}
        
        for main_file in main_files:
            main_stem = Path(main_file).stem  # camera_001_zhuang_1
            
            # æŸ¥æ‰¾å¯¹åº”çš„å·¦ä¸Šè§’å›¾ç‰‡
            for left_file in left_files:
                left_stem = Path(left_file).stem  # camera_001_zhuang_1_left
                if left_stem.startswith(main_stem):
                    mapping[main_file] = left_file
                    break
        
        return mapping
    
    def _extract_position_from_filename(self, filename: str) -> str:
        """ä»æ–‡ä»¶åæå–ä½ç½®ä¿¡æ¯"""
        try:
            # æ–‡ä»¶åæ ¼å¼: camera_001_zhuang_1.png
            parts = filename.split('_')
            if len(parts) >= 4:
                return f"{parts[2]}_{parts[3].split('.')[0]}"
            return "unknown"
        except:
            return "unknown"
    
    def _update_recognition_method_stats(self, camera_id: str, result: Dict[str, Any]):
        """æ›´æ–°è¯†åˆ«æ–¹æ³•ç»Ÿè®¡"""
        try:
            method = result.get('method', 'unknown')
            hybrid_info = result.get('hybrid_info', {})
            used_methods = hybrid_info.get('used_methods', [])
            
            # æ›´æ–°å…¨å±€ç»Ÿè®¡
            if method == 'yolo' and len(used_methods) == 1:
                self.stats['recognition_method_stats']['yolo_complete'] += 1
                self.stats['camera_stats'][camera_id]['recognition_method_counts']['yolo_complete'] += 1
            elif len(used_methods) > 1:
                self.stats['recognition_method_stats']['hybrid_combined'] += 1
                self.stats['camera_stats'][camera_id]['recognition_method_counts']['hybrid_combined'] += 1
            elif 'ocr' in used_methods:
                self.stats['recognition_method_stats']['ocr_only'] += 1
                self.stats['camera_stats'][camera_id]['recognition_method_counts']['ocr_only'] += 1
            elif 'opencv_suit' in used_methods:
                self.stats['recognition_method_stats']['opencv_only'] += 1
                self.stats['camera_stats'][camera_id]['recognition_method_counts']['opencv_only'] += 1
            else:
                self.stats['recognition_method_stats']['failed'] += 1
                
        except Exception:
            pass  # å¿½ç•¥ç»Ÿè®¡é”™è¯¯
    
    def _update_quality_stats(self, camera_id: str, quality_info: Dict[str, Any]):
        """æ›´æ–°è´¨é‡ç»Ÿè®¡"""
        try:
            quality_level = quality_info.get('quality_level', '').lower()
            quality_score = quality_info.get('quality_score', 0.0)
            
            # æ›´æ–°å…¨å±€è´¨é‡ç»Ÿè®¡
            quality_mapping = {
                'ä¼˜ç§€': 'excellent',
                'è‰¯å¥½': 'good', 
                'ä¸€èˆ¬': 'average',
                'è¾ƒå·®': 'poor',
                'å¾ˆå·®': 'very_poor'
            }
            
            for chinese, english in quality_mapping.items():
                if chinese in quality_level:
                    self.stats['quality_stats'][english] += 1
                    break
            
            # æ›´æ–°æ‘„åƒå¤´è´¨é‡å†å²
            camera_stats = self.stats['camera_stats'][camera_id]
            camera_stats['quality_history'].append(quality_score)
            
            # ä¿æŒå†å²è®°å½•æ•°é‡é™åˆ¶
            if len(camera_stats['quality_history']) > 10:
                camera_stats['quality_history'] = camera_stats['quality_history'][-10:]
            
            # æ›´æ–°å¹³å‡è´¨é‡è¯„åˆ†
            if camera_stats['quality_history']:
                camera_stats['average_quality_score'] = sum(camera_stats['quality_history']) / len(camera_stats['quality_history'])
                
        except Exception:
            pass  # å¿½ç•¥ç»Ÿè®¡é”™è¯¯
    
    def _format_recognition_results_simple(self, camera_id: str, position_results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """ç®€å•æ ¼å¼åŒ–è¯†åˆ«ç»“æœï¼ˆä¸ä½¿ç”¨åˆå¹¶å™¨ï¼‰"""
        positions = {}
        
        # æ ‡å‡†ä½ç½®åˆ—è¡¨
        standard_positions = ['zhuang_1', 'zhuang_2', 'zhuang_3', 'xian_1', 'xian_2', 'xian_3']
        
        for position in standard_positions:
            if position in position_results and position_results[position]['success']:
                result = position_results[position]
                positions[position] = {
                    'suit': result.get('suit', ''),
                    'rank': result.get('rank', ''),
                    'confidence': result.get('confidence', 0.0)
                }
            else:
                positions[position] = {
                    'suit': '',
                    'rank': '',
                    'confidence': 0.0
                }
        
        return {
            'success': True,
            'camera_id': camera_id,
            'positions': positions,
            'timestamp': datetime.now().isoformat(),
            'merge_enabled': False
        }
    
    def _convert_merge_result_to_push_format(self, camera_id: str, merge_result: Dict[str, Any]) -> Dict[str, Any]:
        """å°†åˆå¹¶ç»“æœè½¬æ¢ä¸ºæ¨é€æ ¼å¼"""
        positions = {}
        
        # ä»åˆå¹¶ç»“æœæå–ä½ç½®æ•°æ®
        merge_positions = merge_result.get('positions', {})
        standard_positions = ['zhuang_1', 'zhuang_2', 'zhuang_3', 'xian_1', 'xian_2', 'xian_3']
        
        for position in standard_positions:
            if position in merge_positions and merge_positions[position].get('success', False):
                result = merge_positions[position]
                positions[position] = {
                    'suit': result.get('suit', ''),
                    'rank': result.get('rank', ''),
                    'confidence': result.get('confidence', 0.0)
                }
            else:
                positions[position] = {
                    'suit': '',
                    'rank': '',
                    'confidence': 0.0
                }
        
        return {
            'success': True,
            'camera_id': camera_id,
            'positions': positions,
            'timestamp': merge_result.get('timestamp', datetime.now().isoformat()),
            'merge_enabled': True,
            'quality': merge_result.get('quality', {}),
            'summary': merge_result.get('summary', {}),
            'warnings': merge_result.get('warnings', [])
        }
    
    def _push_to_recognition_manager(self, formatted_results: Dict[str, Any]) -> Dict[str, Any]:
        """æ¨é€åˆ°è¯†åˆ«ç»“æœç®¡ç†å™¨"""
        try:
            # ä½¿ç”¨è¯†åˆ«ç»“æœç®¡ç†å™¨
            from src.core.recognition_manager import receive_recognition_data
            
            result = receive_recognition_data(formatted_results)
            
            if result['status'] == 'success':
                return {'success': True, 'message': 'æ¨é€æˆåŠŸ'}
            else:
                return {'success': False, 'message': result.get('message', 'æ¨é€å¤±è´¥')}
                
        except Exception as e:
            return {'success': False, 'message': str(e)}
    
    def run_main_loop(self):
        """è¿è¡Œä¸»å¾ªç¯"""
        try:
            print(f"\nğŸ”„ å¼€å§‹å¢å¼ºç‰ˆå®æ—¶è¯†åˆ«æ¨é€å¾ªç¯")
            print(f"   è¯†åˆ«é—´éš”: {self.config['recognition_interval']} ç§’")
            print(f"   åˆ‡æ¢å»¶è¿Ÿ: {self.config['camera_switch_delay']} ç§’")
            print(f"   å¯ç”¨æ‘„åƒå¤´: {len(self.enabled_cameras)} ä¸ª")
            print(f"   èåˆç­–ç•¥: {self.recognition_config['fusion_strategy']}")
            print(f"   ç»“æœåˆå¹¶: {'å¯ç”¨' if self.config['enable_result_merging'] else 'ç¦ç”¨'}")
            print("=" * 60)
            
            while not self.shutdown_requested:
                cycle_start_time = time.time()
                self.stats['total_cycles'] += 1
                
                # æ˜¾ç¤ºå¾ªç¯ä¿¡æ¯
                self._display_cycle_header()
                
                # è½®è¯¢å¤„ç†æ¯ä¸ªæ‘„åƒå¤´
                for i, camera in enumerate(self.enabled_cameras):
                    if self.shutdown_requested:
                        break
                    
                    camera_id = camera['id']
                    camera_name = camera.get('name', f'æ‘„åƒå¤´{camera_id}')
                    
                    # æ˜¾ç¤ºå½“å‰å¤„ç†çš„æ‘„åƒå¤´
                    self._display_camera_processing(i + 1, len(self.enabled_cameras), camera_name, camera_id)
                    
                    # æ‰§è¡Œå®Œæ•´æµç¨‹
                    success = self._process_single_camera_workflow(camera_id)
                    
                    # æ‘„åƒå¤´åˆ‡æ¢å»¶è¿Ÿ
                    if i < len(self.enabled_cameras) - 1 and not self.shutdown_requested:
                        time.sleep(self.config['camera_switch_delay'])
                
                # æ˜¾ç¤ºæœ¬è½®ç»Ÿè®¡
                cycle_duration = time.time() - cycle_start_time
                self._display_cycle_summary(cycle_duration)
                
                # å¾ªç¯é—´éš”
                remaining_time = max(0, self.config['recognition_interval'] - cycle_duration)
                if remaining_time > 0 and not self.shutdown_requested:
                    self._display_waiting(remaining_time)
                    time.sleep(remaining_time)
                    
        except KeyboardInterrupt:
            print("\nâ¹ï¸  æ¥æ”¶åˆ°åœæ­¢ä¿¡å·")
            self.shutdown_requested = True
        except Exception as e:
            print(f"\nâŒ ä¸»å¾ªç¯å¼‚å¸¸: {e}")
            self.shutdown_requested = True
    
    def _process_single_camera_workflow(self, camera_id: str) -> bool:
        """å¤„ç†å•ä¸ªæ‘„åƒå¤´çš„å®Œæ•´å·¥ä½œæµç¨‹"""
        workflow_start_time = time.time()
        
        try:
            # æ­¥éª¤2: æ‹ç…§
            photo_result = self.step2_take_photo(camera_id)
            
            if not photo_result['success']:
                self._display_step_result("æ‹ç…§", False, photo_result['error'], photo_result['duration'])
                return False
            
            self._display_step_result("æ‹ç…§", True, f"{photo_result['filename']} ({photo_result['file_size']/1024:.1f}KB)", photo_result['duration'])
            
            # æ­¥éª¤3: è£å‰ª
            crop_result = self.step3_crop_images(photo_result['file_path'])
            
            if not crop_result['success']:
                self._display_step_result("è£å‰ª", False, crop_result['error'], crop_result['duration'])
                return False
            
            self._display_step_result("è£å‰ª", True, f"{crop_result['total_count']} ä¸ªåŒºåŸŸ", crop_result['duration'])
            
            # æ­¥éª¤4: æ··åˆè¯†åˆ«
            recognition_result = self.step4_recognize_images_hybrid(
                camera_id, crop_result['main_files'], crop_result['left_files']
            )
            
            if not recognition_result['success']:
                self._display_step_result("æ··åˆè¯†åˆ«", False, recognition_result.get('error', 'è¯†åˆ«å¤±è´¥'), recognition_result['duration'])
                return False
            
            # æ˜¾ç¤ºè¯†åˆ«ç»“æœ
            self._display_recognition_results(camera_id, recognition_result)
            
            # æ­¥éª¤5: åˆå¹¶ç»“æœ
            merge_result = self.step5_merge_results(camera_id, recognition_result['position_results'])
            
            if not merge_result['success']:
                self._display_step_result("ç»“æœåˆå¹¶", False, merge_result.get('error', 'åˆå¹¶å¤±è´¥'), merge_result.get('duration', 0))
                return False
            
            # æ˜¾ç¤ºåˆå¹¶ç»“æœ
            self._display_merge_results(merge_result)
            
            # ä¿å­˜æœ€æ–°ç»“æœ
            self.stats['last_results'][camera_id] = merge_result
            
            # æ­¥éª¤6: æ¨é€
            push_result = self.step6_push_results(camera_id, merge_result)
            self._display_step_result("æ¨é€", push_result['success'], push_result['message'], push_result['duration'])
            
            # æ˜¾ç¤ºæ€»è€—æ—¶
            total_duration = time.time() - workflow_start_time
            print(f"      ğŸ’« æ€»è€—æ—¶: {total_duration:.2f}ç§’")
            
            return True
            
        except Exception as e:
            print(f"      âŒ å·¥ä½œæµç¨‹å¼‚å¸¸: {e}")
            return False
    
    def _display_cycle_header(self):
        """æ˜¾ç¤ºå¾ªç¯å¤´éƒ¨ä¿¡æ¯"""
        with self.display_lock:
            current_time = datetime.now().strftime('%H:%M:%S')
            print(f"\nğŸ”„ ç¬¬ {self.stats['total_cycles']} è½®å¾ªç¯ ({current_time})")
            print("=" * 60)
    
    def _display_camera_processing(self, current: int, total: int, camera_name: str, camera_id: str):
        """æ˜¾ç¤ºå½“å‰å¤„ç†çš„æ‘„åƒå¤´"""
        with self.display_lock:
            print(f"\nğŸ“· ({current}/{total}) {camera_name} (ID: {camera_id})")
            print("-" * 40)
    
    def _display_step_result(self, step_name: str, success: bool, message: str, duration: float):
        """æ˜¾ç¤ºæ­¥éª¤ç»“æœ"""
        with self.display_lock:
            status_icon = "âœ…" if success else "âŒ"
            print(f"      {status_icon} {step_name}: {message} ({duration:.2f}s)")
    
    def _display_recognition_results(self, camera_id: str, recognition_result: Dict[str, Any]):
        """æ˜¾ç¤ºæ··åˆè¯†åˆ«ç»“æœ"""
        with self.display_lock:
            success_count = recognition_result['successful_count']
            total_count = recognition_result['total_count']
            success_rate = recognition_result['success_rate']
            duration = recognition_result['duration']
            
            print(f"      âœ… æ··åˆè¯†åˆ«: {success_count}/{total_count} æˆåŠŸ ({success_rate:.1f}%) ({duration:.2f}s)")
            
            # æ˜¾ç¤ºå…·ä½“è¯†åˆ«ç»“æœå’Œæ–¹æ³•
            position_results = recognition_result['position_results']
            position_names = {
                'zhuang_1': 'åº„1', 'zhuang_2': 'åº„2', 'zhuang_3': 'åº„3',
                'xian_1': 'é—²1', 'xian_2': 'é—²2', 'xian_3': 'é—²3'
            }
            
            recognized_cards = []
            method_counts = {'yolo': 0, 'hybrid': 0, 'ocr': 0, 'opencv': 0}
            
            for position, result in position_results.items():
                if result['success']:
                    pos_name = position_names.get(position, position)
                    display_name = result.get('display_name', 'N/A')
                    confidence = result.get('confidence', 0)
                    
                    # ç»Ÿè®¡è¯†åˆ«æ–¹æ³•
                    hybrid_info = result.get('hybrid_info', {})
                    used_methods = hybrid_info.get('used_methods', [])
                    
                    if len(used_methods) == 1 and 'yolo' in used_methods:
                        method_counts['yolo'] += 1
                        method_indicator = 'Y'
                    elif len(used_methods) > 1:
                        method_counts['hybrid'] += 1
                        method_indicator = 'H'
                    elif 'ocr' in used_methods:
                        method_counts['ocr'] += 1
                        method_indicator = 'O'
                    elif 'opencv_suit' in used_methods:
                        method_counts['opencv'] += 1
                        method_indicator = 'C'
                    else:
                        method_indicator = '?'
                    
                    recognized_cards.append(f"{pos_name}:{display_name}({confidence:.2f})[{method_indicator}]")
            
            if recognized_cards:
                cards_str = " | ".join(recognized_cards)
                print(f"         ğŸ´ {cards_str}")
                
                # æ˜¾ç¤ºæ–¹æ³•ç»Ÿè®¡
                method_summary = []
                if method_counts['yolo'] > 0:
                    method_summary.append(f"YOLO:{method_counts['yolo']}")
                if method_counts['hybrid'] > 0:
                    method_summary.append(f"æ··åˆ:{method_counts['hybrid']}")
                if method_counts['ocr'] > 0:
                    method_summary.append(f"OCR:{method_counts['ocr']}")
                if method_counts['opencv'] > 0:
                    method_summary.append(f"CV:{method_counts['opencv']}")
                
                if method_summary:
                    print(f"         ğŸ§  æ–¹æ³•: {' | '.join(method_summary)}")
    
    def _display_merge_results(self, merge_result: Dict[str, Any]):
        """æ˜¾ç¤ºåˆå¹¶ç»“æœ"""
        with self.display_lock:
            if merge_result.get('merge_enabled', False):
                duration = merge_result.get('merge_duration', 0)
                print(f"      âœ… ç»“æœåˆå¹¶: å®Œæˆ ({duration:.3f}s)")
                
                # æ˜¾ç¤ºè´¨é‡ä¿¡æ¯
                if 'quality' in merge_result:
                    quality = merge_result['quality']
                    quality_level = quality.get('quality_level', 'N/A')
                    quality_score = quality.get('quality_score', 0)
                    print(f"         ğŸ† è´¨é‡: {quality_level} ({quality_score:.3f})")
                
                # æ˜¾ç¤ºè­¦å‘Š
                warnings = merge_result.get('warnings', [])
                if warnings:
                    print(f"         âš ï¸  è­¦å‘Š: {'; '.join(warnings)}")
            else:
                print(f"      âšª ç»“æœåˆå¹¶: å·²ç¦ç”¨")
    
    def _display_cycle_summary(self, cycle_duration: float):
        """æ˜¾ç¤ºå¾ªç¯æ±‡æ€»"""
        with self.display_lock:
            print(f"\nğŸ“Š æœ¬è½®æ±‡æ€»: è€—æ—¶ {cycle_duration:.2f}ç§’")
            
            # æ˜¾ç¤ºå„æ‘„åƒå¤´çŠ¶æ€
            for camera_id, stats in self.stats['camera_stats'].items():
                camera_name = next((c['name'] for c in self.enabled_cameras if c['id'] == camera_id), camera_id)
                
                success_icon = "âœ…" if stats['successful_recognitions'] > 0 else "âšª"
                last_time = stats.get('last_recognition_time', 'æœªçŸ¥')
                avg_quality = stats.get('average_quality_score', 0)
                
                # è¯†åˆ«æ–¹æ³•ç»Ÿè®¡
                method_counts = stats['recognition_method_counts']
                method_str = f"Y{method_counts['yolo_complete']}H{method_counts['hybrid_combined']}O{method_counts['ocr_only']}C{method_counts['opencv_only']}"
                
                print(f"   {success_icon} {camera_name}: æ‹ç…§{stats['successful_photos']}/{stats['total_attempts']} "
                      f"è¯†åˆ«{stats['successful_recognitions']} æ¨é€{stats['successful_pushes']} "
                      f"è´¨é‡{avg_quality:.2f} æ–¹æ³•[{method_str}] æœ€å:{last_time}")
    
    def _display_waiting(self, wait_time: float):
        """æ˜¾ç¤ºç­‰å¾…ä¿¡æ¯"""
        with self.display_lock:
            print(f"â³ ç­‰å¾… {wait_time:.1f}ç§’ åå¼€å§‹ä¸‹è½®å¾ªç¯...")
    
    def display_final_statistics(self):
        """æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡"""
        try:
            print("\nğŸ“ˆ æœ€ç»ˆè¿è¡Œç»Ÿè®¡")
            print("=" * 50)
            
            # è®¡ç®—è¿è¡Œæ—¶é—´
            total_time = datetime.now() - self.stats['start_time']
            print(f"â° æ€»è¿è¡Œæ—¶é—´: {str(total_time).split('.')[0]}")
            print(f"ğŸ”„ æ€»å¾ªç¯æ•°: {self.stats['total_cycles']}")
            
            # æ˜¾ç¤ºè¯†åˆ«æ–¹æ³•ç»Ÿè®¡
            method_stats = self.stats['recognition_method_stats']
            total_recognitions = sum(method_stats.values())
            if total_recognitions > 0:
                print(f"\nğŸ§  è¯†åˆ«æ–¹æ³•ç»Ÿè®¡ (æ€»è®¡: {total_recognitions} æ¬¡):")
                method_names = {
                    'yolo_complete': 'YOLOå®Œæ•´è¯†åˆ«',
                    'hybrid_combined': 'æ··åˆç»„åˆè¯†åˆ«',
                    'ocr_only': 'ä»…OCRè¯†åˆ«',
                    'opencv_only': 'ä»…OpenCVè¯†åˆ«',
                    'failed': 'è¯†åˆ«å¤±è´¥'
                }
                for method, count in method_stats.items():
                    if count > 0:
                        percentage = (count / total_recognitions) * 100
                        method_name = method_names.get(method, method)
                        print(f"  {method_name}: {count} æ¬¡ ({percentage:.1f}%)")
            
            # æ˜¾ç¤ºè´¨é‡ç»Ÿè®¡
            quality_stats = self.stats['quality_stats']
            total_quality = sum(quality_stats.values())
            if total_quality > 0:
                print(f"\nğŸ† è´¨é‡ç­‰çº§ç»Ÿè®¡ (æ€»è®¡: {total_quality} æ¬¡):")
                quality_names = {
                    'excellent': 'ä¼˜ç§€',
                    'good': 'è‰¯å¥½',
                    'average': 'ä¸€èˆ¬',
                    'poor': 'è¾ƒå·®',
                    'very_poor': 'å¾ˆå·®'
                }
                for level, count in quality_stats.items():
                    if count > 0:
                        percentage = (count / total_quality) * 100
                        level_name = quality_names.get(level, level)
                        print(f"  {level_name}: {count} æ¬¡ ({percentage:.1f}%)")
            
            print(f"\nğŸ“· å„æ‘„åƒå¤´è¯¦ç»†ç»Ÿè®¡:")
            for camera_id, stats in self.stats['camera_stats'].items():
                camera_name = next((c['name'] for c in self.enabled_cameras if c['id'] == camera_id), camera_id)
                
                photo_rate = (stats['successful_photos'] / stats['total_attempts'] * 100) if stats['total_attempts'] > 0 else 0
                
                print(f"   {camera_name} (ID: {camera_id}):")
                print(f"     æ‹ç…§: {stats['successful_photos']}/{stats['total_attempts']} ({photo_rate:.1f}%)")
                print(f"     è¯†åˆ«: {stats['successful_recognitions']} æ¬¡æˆåŠŸ")
                print(f"     æ¨é€: {stats['successful_pushes']} æ¬¡æˆåŠŸ")
                print(f"     å¹³å‡è´¨é‡: {stats['average_quality_score']:.3f}")
                
                # è¯†åˆ«æ–¹æ³•åˆ†å¸ƒ
                method_counts = stats['recognition_method_counts']
                method_items = [f"{k}:{v}" for k, v in method_counts.items() if v > 0]
                if method_items:
                    print(f"     æ–¹æ³•åˆ†å¸ƒ: {', '.join(method_items)}")
            
            print("=" * 50)
            
        except Exception as e:
            print(f"âŒ æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='å¢å¼ºç‰ˆå®æ—¶è¯†åˆ«æ¨é€ç³»ç»Ÿ (æ··åˆè¯†åˆ«å™¨)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python tui.py                           # é»˜è®¤é…ç½®è¿è¡Œ
  python tui.py --interval 5              # è®¾ç½®å¾ªç¯é—´éš”ä¸º5ç§’
  python tui.py --no-push                 # ç¦ç”¨æ¨é€åŠŸèƒ½
  python tui.py --strategy voting         # ä½¿ç”¨æŠ•ç¥¨èåˆç­–ç•¥
  python tui.py --no-merge                # ç¦ç”¨ç»“æœåˆå¹¶
  python tui.py --no-yolo                 # ç¦ç”¨YOLOè¯†åˆ«
  python tui.py --debug                   # å¯ç”¨è°ƒè¯•æ¨¡å¼
        """
    )
    
    parser.add_argument('--interval', type=int, default=3,
                       help='è¯†åˆ«å¾ªç¯é—´éš”(ç§’) (é»˜è®¤: 3)')
    parser.add_argument('--camera-delay', type=float, default=1.0,
                       help='æ‘„åƒå¤´åˆ‡æ¢å»¶è¿Ÿ(ç§’) (é»˜è®¤: 1.0)')
    parser.add_argument('--max-retries', type=int, default=3,
                       help='æœ€å¤§é‡è¯•æ¬¡æ•° (é»˜è®¤: 3)')
    parser.add_argument('--no-push', action='store_true',
                       help='ç¦ç”¨æ¨é€åŠŸèƒ½')
    parser.add_argument('--no-merge', action='store_true',
                       help='ç¦ç”¨ç»“æœåˆå¹¶')
    parser.add_argument('--strategy', choices=['weighted', 'voting', 'priority'], 
                       default='weighted', help='èåˆç­–ç•¥ (é»˜è®¤: weighted)')
    parser.add_argument('--debug', action='store_true',
                       help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--no-yolo', action='store_true',
                       help='ç¦ç”¨YOLOè¯†åˆ«')
    parser.add_argument('--no-ocr', action='store_true',
                       help='ç¦ç”¨OCRè¯†åˆ«')
    parser.add_argument('--no-opencv', action='store_true',
                       help='ç¦ç”¨OpenCVèŠ±è‰²è¯†åˆ«')
    
    return parser.parse_args()

def main():
    """ä¸»å‡½æ•°"""
    try:
        args = parse_arguments()
        
        # åˆ›å»ºå¢å¼ºç‰ˆç³»ç»Ÿå®ä¾‹
        system = EnhancedTuiSystem()
        
        # æ›´æ–°ç³»ç»Ÿé…ç½®
        system.config.update({
            'recognition_interval': args.interval,
            'camera_switch_delay': args.camera_delay,
            'max_retry_times': args.max_retries,
            'enable_websocket': not args.no_push,
            'enable_result_merging': not args.no_merge,
        })
        
        # æ›´æ–°è¯†åˆ«é…ç½®
        system.recognition_config.update({
            'fusion_strategy': args.strategy,
            'debug_mode': args.debug,
            'yolo_enabled': not args.no_yolo,
            'ocr_enabled': not args.no_ocr,
            'opencv_suit_enabled': not args.no_opencv
        })
        
        # æ­¥éª¤1: è¯»å–æ‘„åƒå¤´é…ç½®
        if not system.step1_load_camera_config():
            return 1
        
        # æ˜¾ç¤ºç³»ç»Ÿé…ç½®
        print(f"\nğŸš€ å¢å¼ºç‰ˆç³»ç»Ÿé…ç½®:")
        print(f"   å¾ªç¯é—´éš”: {system.config['recognition_interval']} ç§’")
        print(f"   åˆ‡æ¢å»¶è¿Ÿ: {system.config['camera_switch_delay']} ç§’")
        print(f"   æœ€å¤§é‡è¯•: {system.config['max_retry_times']} æ¬¡")
        print(f"   æ¨é€åŠŸèƒ½: {'å¯ç”¨' if system.config['enable_websocket'] else 'ç¦ç”¨'}")
        print(f"   ç»“æœåˆå¹¶: {'å¯ç”¨' if system.config['enable_result_merging'] else 'ç¦ç”¨'}")
        print(f"   èåˆç­–ç•¥: {system.recognition_config['fusion_strategy']}")
        print(f"   YOLOè¯†åˆ«: {'å¯ç”¨' if system.recognition_config['yolo_enabled'] else 'ç¦ç”¨'}")
        print(f"   OCRè¯†åˆ«: {'å¯ç”¨' if system.recognition_config['ocr_enabled'] else 'ç¦ç”¨'}")
        print(f"   OpenCVè¯†åˆ«: {'å¯ç”¨' if system.recognition_config['opencv_suit_enabled'] else 'ç¦ç”¨'}")
        print(f"   è°ƒè¯•æ¨¡å¼: {'å¯ç”¨' if system.recognition_config['debug_mode'] else 'ç¦ç”¨'}")
        
        # è®¾ç½®ä¿¡å·å¤„ç†
        def signal_handler(signum, frame):
            print(f"\nğŸ“¡ æ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œå‡†å¤‡å…³é—­ç³»ç»Ÿ...")
            system.shutdown_requested = True
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
        print("\nğŸ”„ æŒ‰ Ctrl+C åœæ­¢ç³»ç»Ÿ")
        
        # è¿è¡Œä¸»å¾ªç¯
        system.run_main_loop()
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        if system.stats['total_cycles'] > 0:
            system.display_final_statistics()
        
        print("ğŸ‘‹ å¢å¼ºç‰ˆå®æ—¶è¯†åˆ«æ¨é€ç³»ç»Ÿå·²å…³é—­")
        
        return 0
        
    except Exception as e:
        print(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())